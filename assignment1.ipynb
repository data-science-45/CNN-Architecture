{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling in Convolutional Neural Networks (CNNs) serves the purpose of reducing the spatial dimensions (width and height) of the input volume, thus reducing the amount of parameters and computation in the network, while still preserving the important features. The main types of pooling operations used in CNNs are max pooling and average pooling.\n",
    "\n",
    "Here's a breakdown of the purpose and benefits of pooling in CNNs:\n",
    "\n",
    "Dimensionality Reduction: Pooling reduces the size of the feature maps, which in turn reduces the number of parameters in the network. This helps in controlling overfitting and managing computational complexity.\n",
    "\n",
    "Translation Invariance: Pooling creates a level of translation invariance, meaning that the exact location of the features in the input image becomes less important. This is achieved by retaining the most significant features while discarding the less relevant ones. For example, if a certain feature is detected in a specific region of an image, pooling ensures that the same feature detected in a slightly different location would still produce the same output after pooling.\n",
    "\n",
    "Increased Receptive Field: Pooling helps in increasing the receptive field of neurons in deeper layers of the network. By combining neighboring features into a single representative feature, pooling helps each neuron in the subsequent layer to cover a larger portion of the input image.\n",
    "\n",
    "Computational Efficiency: Pooling reduces the amount of computation required for subsequent layers. By downsampling the feature maps, the subsequent layers have fewer inputs to process, which leads to faster training and inference times.\n",
    "\n",
    "Robustness to Variations: Pooling makes the network more robust to variations in the input, such as small translations, rotations, or distortions. Since pooling summarizes local features, minor changes in the input are less likely to affect the overall output of the network.\n",
    "\n",
    "Feature Generalization: Pooling helps in generalizing the features learned by the network. By aggregating information from neighboring regions, pooling captures the essential characteristics of the features while discarding irrelevant details, which leads to better generalization to unseen data.\n",
    "\n",
    "Overall, pooling plays a crucial role in CNNs by reducing spatial dimensions, controlling overfitting, improving computational efficiency, enhancing translation invariance, increasing receptive field, and promoting feature generalization, ultimately contributing to the network's ability to learn and extract meaningful features from input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Pooling:\n",
    "\n",
    "Max pooling is a pooling operation commonly used in convolutional neural networks.\n",
    "In max pooling, for each local region of the input feature map, the maximum value is taken.\n",
    "It retains the most active features in each region, discarding less relevant information.\n",
    "Max pooling is effective in capturing the most prominent features within each region, aiding in feature detection and invariance to small spatial translations.\n",
    "Min Pooling:\n",
    "\n",
    "Min pooling, on the other hand, is less commonly used compared to max pooling.\n",
    "In min pooling, for each local region of the input feature map, the minimum value is taken.\n",
    "It retains the least active features in each region, discarding more prominent information.\n",
    "Min pooling might be used in scenarios where the goal is to focus on less intense features or to emphasize the weaker activations within the data.\n",
    "Differences:\n",
    "\n",
    "The primary difference lies in the operation applied to each local region of the input feature map. Max pooling takes the maximum value, while min pooling takes the minimum value.\n",
    "Max pooling is more commonly used and is effective in capturing the most prominent features, while min pooling might be used in specific scenarios where emphasizing weaker activations or less intense features is desired.\n",
    "Due to its emphasis on capturing prominent features, max pooling is more prevalent in convolutional neural network architectures and is typically the default choice for pooling operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding in Convolutional Neural Networks (CNNs) is a technique used to preserve the spatial dimensions of the input volume, especially around the edges, when applying convolutional operations. It involves adding additional pixels around the input image or feature map.\n",
    "\n",
    "Here's a discussion of the concept of padding in CNNs and its significance:\n",
    "\n",
    "Preservation of Spatial Information:\n",
    "\n",
    "Without padding, as convolutional layers progress through the network, the spatial dimensions of the feature maps tend to shrink. This can result in the loss of important spatial information, especially around the borders of the input image.\n",
    "Padding helps maintain the spatial dimensions of the feature maps by adding extra pixels around the borders. This ensures that the convolutional filters can process information near the edges of the input volume.\n",
    "Controlling Output Size:\n",
    "\n",
    "Padding allows for more control over the size of the output feature maps after convolution. By adjusting the amount of padding added, one can control how much the feature maps are shrunk during convolution.\n",
    "For example, if 'valid' padding (no padding) is used, the output feature maps will be smaller than the input. However, with appropriate padding, the output feature maps can be kept the same size as the input or even increased in size.\n",
    "Boundary Effects and Edge Information:\n",
    "\n",
    "The pixels at the edges of an image contain important information, especially in tasks like object detection and segmentation. Padding helps to preserve this edge information by allowing the convolutional filters to consider the pixels near the image boundaries.\n",
    "Without padding, the information at the edges might be underutilized, leading to suboptimal performance in tasks where accurate boundary detection is crucial.\n",
    "Symmetry and Centering:\n",
    "\n",
    "Padding ensures that the convolutional filters are symmetrically applied across the input volume. This is important for tasks like feature detection, where symmetry helps in detecting patterns regardless of their position within the image.\n",
    "Additionally, padding helps to center the convolutional filters on each pixel of the input volume, ensuring that the features are extracted uniformly across the entire image.\n",
    "Stability and Regularization:\n",
    "\n",
    "Padding can also improve the stability of training by reducing the likelihood of vanishing gradients or exploding gradients, especially in deeper networks. It provides a buffer zone around the edges, allowing for smoother gradients during backpropagation.\n",
    "Moreover, padding can act as a form of regularization by preventing overfitting, as it introduces additional information to the network during training.\n",
    "In summary, padding in CNNs plays a crucial role in preserving spatial information, controlling the output size, maintaining edge information, ensuring symmetry and centering, and improving the stability and regularization of the training process. It is an essential technique for achieving better performance and accuracy in various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-padding:\n",
    "\n",
    "Zero-padding involves adding a border of zeros around the input image or feature map before applying convolution.\n",
    "In zero-padding, the extra border of zeros ensures that the output feature map has the same spatial dimensions (height and width) as the input, regardless of the size of the convolutional filter or the stride.\n",
    "For example, if a 3x3 convolutional filter is applied to a 5x5 input feature map with zero-padding, the resulting feature map will also be 5x5.\n",
    "Constant Zero-padding:\n",
    "\n",
    "Constant zero-padding is similar to zero-padding but allows specifying a value other than zero for padding.\n",
    "It involves adding a border of a constant value (usually specified by the user) around the input image or feature map before convolution.\n",
    "Like zero-padding, constant zero-padding ensures that the output feature map maintains the same spatial dimensions as the input.\n",
    "However, unlike zero-padding where only zeros are added, constant zero-padding allows for the customization of the padding value.\n",
    "Valid-padding:\n",
    "\n",
    "Valid-padding (also known as no-padding) involves applying convolution directly to the input image or feature map without adding any extra border.\n",
    "When using valid-padding, the convolution operation is only performed on positions where the filter and the input overlap completely.\n",
    "As a result, the output feature map will have reduced spatial dimensions compared to the input, depending on the size of the filter and the stride.\n",
    "For example, if a 3x3 filter is applied to a 5x5 input feature map with valid-padding and a stride of 1, the resulting feature map will be 3x3.\n",
    "Comparison:\n",
    "\n",
    "Zero-padding and constant zero-padding both maintain the spatial dimensions of the output feature map, ensuring that it has the same size as the input.\n",
    "Valid-padding, on the other hand, results in a smaller output feature map compared to the input due to the absence of padding.\n",
    "Zero-padding and constant zero-padding are commonly used to preserve spatial information, maintain symmetry, and prevent information loss, especially at the edges of the input.\n",
    "Valid-padding is often used when reducing the spatial dimensions of the feature map is desired or when the network architecture requires it, such as in cases where downsampling is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer:\n",
    "\n",
    "LeNet-5 takes as input grayscale images of size 32x32 pixels.\n",
    "First Convolutional Layer (C1):\n",
    "\n",
    "The first convolutional layer applies six filters of size 5x5 to the input image.\n",
    "Each filter produces a feature map by convolving with the input image.\n",
    "The activation function used is the hyperbolic tangent (tanh).\n",
    "The output feature maps have a size of 28x28x6.\n",
    "First Subsampling Layer (S2):\n",
    "\n",
    "Following the first convolutional layer, LeNet-5 employs subsampling (average pooling) with a 2x2 kernel and a stride of 2.\n",
    "Subsampling reduces the spatial dimensions of the feature maps, resulting in an output size of 14x14x6.\n",
    "Second Convolutional Layer (C3):\n",
    "\n",
    "The second convolutional layer applies sixteen filters of size 5x5 to the feature maps from the first subsampling layer (S2).\n",
    "Similar to the first convolutional layer, each filter produces a feature map.\n",
    "Again, the activation function used is the hyperbolic tangent (tanh).\n",
    "The output feature maps have a size of 10x10x16.\n",
    "Second Subsampling Layer (S4):\n",
    "\n",
    "After the second convolutional layer, another subsampling (average pooling) layer is applied with a 2x2 kernel and a stride of 2.\n",
    "Subsampling reduces the spatial dimensions further, resulting in an output size of 5x5x16.\n",
    "Fully Connected Layers (F5 and F6):\n",
    "\n",
    "The feature maps from the second subsampling layer (S4) are flattened into a vector.\n",
    "The flattened vector serves as input to two fully connected layers.\n",
    "The first fully connected layer (F5) consists of 120 neurons, each connected to every element of the flattened vector.\n",
    "The activation function used is the hyperbolic tangent (tanh).\n",
    "The second fully connected layer (F6) consists of 84 neurons.\n",
    "Again, the activation function used is the hyperbolic tangent (tanh).\n",
    "Output Layer:\n",
    "\n",
    "The final fully connected layer produces the output logits, representing the class scores for the input image.\n",
    "The output layer typically uses a softmax activation function to produce class probabilities.\n",
    "Overall, LeNet-5 was a groundbreaking architecture for its time, demonstrating the effectiveness of convolutional neural networks (CNNs) for tasks like handwritten digit recognition. It laid the foundation for modern CNN architectures and paved the way for the widespread adoption of deep learning in computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layers (C1 and C3):\n",
    "\n",
    "The LeNet-5 architecture consists of two convolutional layers: C1 and C3.\n",
    "These layers apply learnable filters (kernels) to extract features from the input images.\n",
    "Each filter convolves with the input image to produce feature maps.\n",
    "The purpose of the convolutional layers is to capture spatial hierarchies of patterns and features within the input images. These features could include edges, corners, and other basic shapes that are relevant to the task at hand.\n",
    "Subsampling Layers (S2 and S4):\n",
    "\n",
    "After each convolutional layer, LeNet-5 employs subsampling layers, also known as pooling layers: S2 and S4.\n",
    "Subsampling layers reduce the spatial dimensions of the feature maps produced by the convolutional layers.\n",
    "This reduction is typically achieved through techniques like average pooling or max pooling.\n",
    "The purpose of subsampling is to enhance computational efficiency, reduce the sensitivity to small translations in the input images, and progressively aggregate the most important features.\n",
    "Fully Connected Layers (F5 and F6):\n",
    "\n",
    "Following the convolutional and subsampling layers, LeNet-5 includes two fully connected layers: F5 and F6.\n",
    "Fully connected layers connect every neuron in one layer to every neuron in the next layer, forming a densely connected neural network.\n",
    "The purpose of fully connected layers is to perform high-level feature extraction and classification.\n",
    "These layers take the flattened output of the preceding layers and learn complex patterns and relationships within the extracted features.\n",
    "The final fully connected layer produces the output logits, which are used to make predictions about the input data.\n",
    "Activation Functions:\n",
    "\n",
    "Throughout the network, LeNet-5 uses the hyperbolic tangent (tanh) activation function.\n",
    "Tanh squashes the input values to the range [-1, 1], allowing the network to capture both positive and negative information.\n",
    "The purpose of activation functions is to introduce nonlinearity into the network, enabling it to learn complex mappings between the input and output.\n",
    "Output Layer:\n",
    "\n",
    "The output layer of LeNet-5 produces the final predictions or class scores for the input data.\n",
    "In the case of LeNet-5, the output layer typically employs a softmax activation function to produce class probabilities.\n",
    "The purpose of the output layer is to provide a probabilistic interpretation of the network's predictions, indicating the likelihood of each class given the input data.\n",
    "These components work together synergistically to enable LeNet-5 to effectively extract features from input images and make accurate predictions for tasks such as handwritten digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! LeNet-5 was a groundbreaking architecture at its time and laid the foundation for modern convolutional neural networks (CNNs). However, like any other model, it has its own set of advantages and limitations, especially in the context of image classification tasks. Let's discuss them:\n",
    "\n",
    "Advantages of LeNet-5:\n",
    "\n",
    "Effective Feature Extraction: LeNet-5 demonstrated the effectiveness of convolutional layers in extracting hierarchical features from images. Its architecture allowed it to capture low-level features like edges and textures in the early layers, gradually building up to more abstract features in deeper layers.\n",
    "\n",
    "Translation Invariance: The use of subsampling layers (pooling) in LeNet-5 helped in achieving translation invariance, meaning the network could recognize patterns regardless of their exact location in the input image. This property is particularly useful in tasks where the position of objects may vary.\n",
    "\n",
    "Efficient Architecture: LeNet-5 had a relatively simple architecture compared to modern CNNs, making it computationally efficient. It required fewer parameters and computations, which made it feasible to train even on the hardware available at the time of its development.\n",
    "\n",
    "Pioneering Work: LeNet-5 paved the way for further research in deep learning, particularly in the field of computer vision. Its success demonstrated the potential of neural networks for image recognition tasks, sparking interest and investment in the development of more sophisticated architectures.\n",
    "\n",
    "Limitations of LeNet-5:\n",
    "\n",
    "Limited Capacity: Compared to modern CNN architectures, LeNet-5 has a relatively shallow architecture with fewer layers and parameters. This limited capacity may hinder its ability to learn complex patterns and representations, leading to suboptimal performance on challenging datasets.\n",
    "\n",
    "Small Receptive Field: Due to its small filter sizes and limited depth, LeNet-5 has a small receptive field, which means it may struggle to capture global context and long-range dependencies in images. This limitation can affect its performance on tasks that require understanding of broader spatial relationships.\n",
    "\n",
    "Dependence on Handcrafted Features: LeNet-5 relies on manually designed features learned through convolutional and pooling operations. While effective for simple tasks like handwritten digit recognition, this approach may not generalize well to more complex datasets with diverse classes and variations.\n",
    "\n",
    "Sensitivity to Input Size: LeNet-5 was designed to work with small input images (32x32 pixels). While suitable for the MNIST dataset, it may struggle with larger and more detailed images commonly encountered in real-world applications. Scaling up LeNet-5 to handle larger images may require significant modifications.\n",
    "\n",
    "In summary, while LeNet-5 was a pioneering CNN architecture with several advantages, such as effective feature extraction and computational efficiency, it also has limitations, such as limited capacity and dependence on handcrafted features. It serves as a foundation for subsequent developments in deep learning but may not be suitable for tackling more complex image classification tasks without modifications or enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape input images to match LeNet-5 input size (32x32)\n",
    "x_train = tf.image.resize_with_pad(x_train, 32, 32)\n",
    "x_test = tf.image.resize_with_pad(x_test, 32, 32)\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='tanh'),\n",
    "    layers.Dense(84, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_labels = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Accuracy Score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet is a pioneering convolutional neural network (CNN) architecture developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, significantly advancing the state-of-the-art in image classification tasks. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "Input Layer:\n",
    "\n",
    "AlexNet takes as input color images of size 227x227 pixels with three color channels (RGB).\n",
    "Convolutional Layers:\n",
    "\n",
    "AlexNet consists of five convolutional layers, denoted as Conv1 through Conv5.\n",
    "The first convolutional layer (Conv1) applies 96 filters of size 11x11 with a stride of 4 pixels.\n",
    "The subsequent convolutional layers (Conv2-Conv5) apply smaller filters (3x3 or 5x5) with varying numbers of filters.\n",
    "ReLU Activation and Local Response Normalization (LRN):\n",
    "\n",
    "After each convolutional layer, AlexNet applies the rectified linear unit (ReLU) activation function.\n",
    "Additionally, local response normalization (LRN) is applied after the first and second convolutional layers to enhance the model's generalization capabilities.\n",
    "Max Pooling Layers:\n",
    "\n",
    "After the first, second, and fifth convolutional layers, AlexNet includes max pooling layers.\n",
    "Max pooling is performed over 3x3 pixel windows with a stride of 2 pixels, reducing the spatial dimensions of the feature maps.\n",
    "Fully Connected Layers:\n",
    "\n",
    "Following the convolutional and pooling layers, AlexNet includes three fully connected layers denoted as FC6, FC7, and FC8.\n",
    "The first two fully connected layers (FC6 and FC7) consist of 4096 neurons each, while the final fully connected layer (FC8) produces the output logits for classification.\n",
    "Dropout regularization is applied to FC6 and FC7 layers during training to prevent overfitting.\n",
    "Softmax Output Layer:\n",
    "\n",
    "The output layer (FC8) employs a softmax activation function to produce class probabilities for the input image.\n",
    "AlexNet is typically trained for classification tasks with 1000 output classes, corresponding to the ImageNet dataset.\n",
    "Training:\n",
    "\n",
    "AlexNet is trained using stochastic gradient descent (SGD) with momentum, along with weight decay regularization.\n",
    "Data augmentation techniques such as random cropping and horizontal flipping are applied to the input images to increase the diversity of the training data and improve generalization.\n",
    "Parallelism and GPU Acceleration:\n",
    "\n",
    "AlexNet was one of the first deep neural networks to leverage the computational power of graphics processing units (GPUs) for training.\n",
    "It introduced the concept of parallelism by distributing the workload across two GPUs, significantly reducing training time.\n",
    "Overall, AlexNet's architecture introduced several key innovations, including the use of deep convolutional layers, ReLU activation, local response normalization, dropout regularization, and GPU acceleration. It demonstrated the effectiveness of deep learning in image classification tasks and paved the way for subsequent advancements in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet introduced several architectural innovations that contributed to its breakthrough performance in image classification tasks. These innovations addressed key challenges in deep learning and significantly improved the model's performance. Here are the architectural innovations introduced in AlexNet:\n",
    "\n",
    "Deep Convolutional Layers:\n",
    "\n",
    "AlexNet featured a deep architecture with multiple convolutional layers. Prior to AlexNet, neural networks were relatively shallow due to computational constraints.\n",
    "Deeper networks allow for the extraction of more abstract and hierarchical features from the input images, leading to better representation learning.\n",
    "Rectified Linear Units (ReLU) Activation Function:\n",
    "\n",
    "AlexNet used the rectified linear unit (ReLU) activation function instead of traditional sigmoid or hyperbolic tangent functions.\n",
    "ReLU introduces non-linearity to the network while being computationally efficient. It helps alleviate the vanishing gradient problem and accelerates convergence during training.\n",
    "Local Response Normalization (LRN):\n",
    "\n",
    "AlexNet incorporated local response normalization (LRN) after the first and second convolutional layers.\n",
    "LRN helps in generalization by normalizing the responses across adjacent channels within the same spatial location. It enhances the model's ability to discriminate between different features.\n",
    "Overlapping Max Pooling:\n",
    "\n",
    "In addition to traditional max pooling, AlexNet used overlapping max pooling with a stride smaller than the pooling size.\n",
    "Overlapping max pooling helps in preserving spatial information while reducing the spatial dimensions of the feature maps. It prevents overfitting and improves translation invariance.\n",
    "Dropout Regularization:\n",
    "\n",
    "AlexNet employed dropout regularization in the fully connected layers (FC6 and FC7) during training.\n",
    "Dropout randomly drops out a fraction of neurons during each training iteration, preventing co-adaptation of neurons and reducing overfitting.\n",
    "GPU Acceleration and Parallelism:\n",
    "\n",
    "AlexNet was one of the first deep learning models to leverage the computational power of graphics processing units (GPUs) for training.\n",
    "It exploited parallelism by distributing the workload across multiple GPUs, significantly reducing training time and enabling the training of deeper models.\n",
    "Large-Scale Dataset and Data Augmentation:\n",
    "\n",
    "AlexNet was trained on the large-scale ImageNet dataset with millions of labeled images and thousands of classes.\n",
    "It utilized data augmentation techniques such as random cropping and horizontal flipping to increase the diversity of the training data and improve generalization.\n",
    "These architectural innovations collectively contributed to AlexNet's breakthrough performance in image classification tasks, enabling it to achieve state-of-the-art results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. AlexNet's success demonstrated the potential of deep learning and paved the way for the development of more sophisticated convolutional neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AlexNet, the convolutional layers, pooling layers, and fully connected layers play crucial roles in extracting features from input images and making predictions. Here's a discussion of the role of each of these layers in the AlexNet architecture:\n",
    "\n",
    "Convolutional Layers:\n",
    "\n",
    "AlexNet includes five convolutional layers denoted as Conv1 through Conv5.\n",
    "These convolutional layers apply learnable filters to extract features from the input images.\n",
    "The first convolutional layer (Conv1) applies 96 filters of size 11x11 to the input images with a stride of 4 pixels. This layer captures low-level features such as edges and textures.\n",
    "Subsequent convolutional layers (Conv2 through Conv5) use smaller filter sizes (3x3 or 5x5) and a stride of 1 pixel to capture higher-level features and spatial hierarchies of patterns.\n",
    "The depth of the network allows it to learn complex and abstract representations of the input images, enabling better discrimination between different classes.\n",
    "Pooling Layers:\n",
    "\n",
    "AlexNet employs max pooling layers after the first, second, and fifth convolutional layers.\n",
    "Max pooling is performed over 3x3 pixel windows with a stride of 2 pixels, reducing the spatial dimensions of the feature maps.\n",
    "Pooling layers help in achieving translation invariance, reducing the sensitivity of the network to small variations in the input images.\n",
    "By downsampling the feature maps, pooling layers also increase the receptive field of the neurons in deeper layers, enabling them to capture larger spatial contexts.\n",
    "Fully Connected Layers:\n",
    "\n",
    "Following the convolutional and pooling layers, AlexNet includes three fully connected layers: FC6, FC7, and FC8.\n",
    "The fully connected layers serve as high-level feature extractors and classifiers, capturing global patterns and relationships in the extracted features.\n",
    "The first two fully connected layers (FC6 and FC7) consist of 4096 neurons each, followed by the final fully connected layer (FC8) with 1000 neurons corresponding to the 1000 classes in the ImageNet dataset.\n",
    "Dropout regularization is applied to FC6 and FC7 layers during training to prevent overfitting by randomly dropping out a fraction of neurons.\n",
    "In summary, the convolutional layers in AlexNet are responsible for feature extraction, capturing both low-level and high-level features from the input images. Pooling layers help in downsampling the feature maps and achieving translation invariance, while fully connected layers serve as classifiers, making predictions based on the extracted features. Together, these layers enable AlexNet to effectively learn hierarchical representations of input images and achieve state-of-the-art performance in image classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define AlexNet architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(256, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_labels = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Accuracy Score:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
